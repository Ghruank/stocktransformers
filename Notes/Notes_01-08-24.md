# Notes 01-08-24

## Week 3 Course 5

### Machine Translation

* For Machine Translation we can utilize Many tom Many LSTM architecture with an encoder and decoder network. Unlike Language model, Machine Translatiom model should sample random output as it would require the best output.
* We cannot go with a greedy search approach as it does not work well on Machine Translation. Greedy Search would give the bes prediction of the next word in enxt sequence however we need the probabilty of the whole sequence to be best which is not neccesary with this approach

### Beam Search
* Thus, we utilize beam search approach wherein first we pick the most probable 'n'(beam width) output and then calculate the conditional probability of the next sequence for this 'n' output and again pick the n most porbable output.
* One of the problens with this is that while mutliplying probabilities which are small fractions can cause numerical overflow. Thus, instead of directly multiplying the probabilities we sum up the the log of that. 
* But this causes preferenes to shorter sentences. Thus to remove this bias we divide it by length ofthe sequence.Usually, it is also rasied to power (alpha)
* In practice the size of beam search is of usally 10. However, size of 100 or 1000 is not uncommon.
* Error in a machine Translatiom model can be because of two reason : RNN or Beam Search. Thus, to identify what we need to optimize we should first calculate the the output probability of both the correct sequence and the the output that we have got. 
* If probability of correct sentence is more it implies RNN is not at fault, thus we need to increase beam width. ELse, the RNNmodel is at fault and we need to som changes in our model.

### 